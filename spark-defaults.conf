spark.hadoop.fs.s3a.connection.maximum=100
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
# 예상보다 큰 프레임을 처리하려고 시도하면서 발생한 오류 해결 코드 
spark.network.maxRemoteBlockSizeFetchToMem 1024m
spark.io.file.buffer 65536
# parquet 파일을 여러번 읽거나 병렬 처리할 때, 메모리 사용이 급격히 증가할 수 있음 
spark.sql.parquet.mergeSchema=false
spark.sql.files.maxPartitionBytes=128MB
